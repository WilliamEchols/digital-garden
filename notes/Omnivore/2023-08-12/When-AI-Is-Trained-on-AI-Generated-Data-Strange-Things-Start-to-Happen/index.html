<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="When AI Is Trained on AI-Generated Data, Strange Things Start to Happen #Omnivore
 Read on Omnivore Read Original
State #Omnivore/inbox"><meta property="og:title" content><meta property="og:description" content="When AI Is Trained on AI-Generated Data, Strange Things Start to Happen #Omnivore
 Read on Omnivore Read Original
State #Omnivore/inbox"><meta property="og:type" content="website"><meta property="og:image" content="https://williamechols.github.io/icon.png"><meta property="og:url" content="https://williamechols.github.io/notes/Omnivore/2023-08-12/When-AI-Is-Trained-on-AI-Generated-Data-Strange-Things-Start-to-Happen/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="When AI Is Trained on AI-Generated Data, Strange Things Start to Happen #Omnivore
 Read on Omnivore Read Original
State #Omnivore/inbox"><meta name=twitter:image content="https://williamechols.github.io/icon.png"><title>William Echols</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://williamechols.github.io//icon.png><link href=https://williamechols.github.io/styles.bd7a6f095647daf6971ccf2ac96c99f9.min.css rel=stylesheet><link href=https://williamechols.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://williamechols.github.io/js/darkmode.6850d952ee1e4e9bd5f7d9a584c84c4c.min.js></script>
<script src=https://williamechols.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://williamechols.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://williamechols.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://williamechols.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://williamechols.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://williamechols.github.io/",fetchData=Promise.all([fetch("https://williamechols.github.io/indices/linkIndex.7ab95d3787dc707de4a64056480b2b6c.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://williamechols.github.io/indices/contentIndex.5f800c693627b79e836f0fd414434435.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://williamechols.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!0;drawGraph("https://williamechols.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:3,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:.7,repelForce:.1,scale:1}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/williamechols.github.io\/js\/router.4a1af5d56de3173257cb6102b67008ea.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=williamechols.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://williamechols.github.io/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://williamechols.github.io/>William Echols</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">s</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div><div id=color-picker><input type=color id=secondary-color name=secondary-color value=#16A085></div></header><script>var element=document.getElementById("secondary-color");element&&element.addEventListener("change",function(){document.documentElement.style.setProperty("--secondary",this.value)})</script><article><p class=meta>Last updated
Aug 17, 2023</p><ul class=tags></ul><a href=#when-ai-is-trained-on-ai-generated-data-strange-things-start-to-happen><h1 id=when-ai-is-trained-on-ai-generated-data-strange-things-start-to-happen><span class=hanchor arialabel=Anchor># </span>When AI Is Trained on AI-Generated Data, Strange Things Start to Happen</h1></a><p>#Omnivore</p><p><a href=https://omnivore.app/me/when-ai-is-trained-on-ai-generated-data-strange-things-start-to--189ebe37dad rel=noopener>Read on Omnivore</a>
<a href=https://futurism.com/ai-trained-ai-generated-data-interview rel=noopener>Read Original</a></p><a href=#state><h1 id=state><span class=hanchor arialabel=Anchor># </span>State</h1></a><p>#Omnivore/inbox</p><a href=#content><h1 id=content><span class=hanchor arialabel=Anchor># </span>Content</h1></a><a href=#loops-upon-loops><h2 id=loops-upon-loops><span class=hanchor arialabel=Anchor># </span>&ldquo;Loops upon loops.&rdquo;</h2></a><p><img src="https://proxy-prod.omnivore-image-cache.app/0x0,sPOHhTH9_XarDUMwZ4lWvuDT3toEVpYw4DiWqLBFxExY/https://futurism.com/_next/image?url=https%3A%2F%2Fwp-assets.futurism.com%2F2023%2F08%2Fai-trained-ai-generated-data.jpg&w=2048&q=75" width=auto alt="Getty / Futurism" title="Getty / Futurism"></p><p>Image by</p><p>Getty / Futurism</p><p>It hasn&rsquo;t even been a year since OpenAI released ChatGPT, and already generative AI is everywhere. It&rsquo;s in
<a href=https://futurism.com/college-student-caught-writing-paper-chatgpt rel=noopener>classrooms</a>; it&rsquo;s in
<a href=https://futurism.com/the-byte/republlications-ai-generated-ad rel=noopener>political advertisements</a>; it&rsquo;s in
<a href=https://futurism.com/studios-ai-replace-background-actors rel=noopener>entertainment</a> and
<a href=https://futurism.com/cnet-ai-errors rel=noopener>journalism</a> and a growing number of
<a href=https://futurism.com/content-farms-ai rel=noopener>AI-powered content farms</a>. Hell, generative AI has even been integrated
<a href=https://futurism.com/google-ai-search-journalism rel=noopener>into search engines</a>, the great mediators and organizers of the open web. People have
<a href=https://futurism.com/the-byte/copywriter-fired-chatgpt rel=noopener>already lost work</a> to the tech, while new and often confounding
<a href=https://futurism.com/the-byte/netflix-cash-in-ai rel=noopener>AI-related careers</a> seem to be on the rise.</p><p>Though whether it sticks in the long term remains to be seen, at least for the time being generative AI seems to be cementing its place in our digital <em>and</em> real lives. And as it becomes increasingly ubiquitous, so does the synthetic content it produces. But in an ironic twist, those same synthetic outputs
<a href=https://futurism.com/ai-trained-ai-generated-data rel=noopener>might also stand to be generative AI&rsquo;s biggest threat.</a></p><p>That&rsquo;s because underpinning the growing generative AI economy is human-made data. Generative AI models don&rsquo;t just cough up human-like content out of thin air; they&rsquo;ve been trained to do so using troves of material that actually <em>was</em> made by humans, usually scraped from the web. But as it turns out, when you feed synthetic content back to a generative AI model, strange things start to happen. Think of it like data inbreeding, leading to increasingly mangled, bland, and all-around bad outputs. (Back in February, Monash University data researcher Jathan Sadowski
<a href=https://twitter.com/jathansadowski/status/1625245803211272194 rel=noopener>described it</a> as &ldquo;Habsburg AI,&rdquo; or &ldquo;a system that is so heavily trained on the outputs of other generative AI&rsquo;s that it becomes an inbred mutant, likely with exaggerated, grotesque features.&rdquo;)</p><p>It&rsquo;s a problem that looms large. AI builders are continuously hungry to feed their models more data, which is generally being scraped from an internet that&rsquo;s increasingly laden with synthetic content. If there&rsquo;s too much destructive inbreeding, could everything just&mldr; fall apart?</p><p>To understand this phenomenon better, we spoke to machine learning researchers Sina Alemohammad and Josue Casco-Rodriguez, both PhD students in Rice University&rsquo;s Electrical and Computer Engineering department, and their supervising professor, Richard G. Baraniuk. In collaboration with researchers at Stanford, they recently published a fascinating â€” though yet to be peer-reviewed â€” paper on the subject, titled &ldquo;
<a href=https://arxiv.org/pdf/2307.01850.pdf rel=noopener>Self-Consuming Generative Models Go MAD</a>.&rdquo;</p><p>MAD, which stands for Model Autophagy Disorder, is the term that they&rsquo;ve coined for AI&rsquo;s apparent self-allergy. In their research, it took only five cycles of training on synthetic data for an AI model&rsquo;s outputs to, in the words of Baraniuk, &ldquo;blow up.&rdquo;</p><p>It&rsquo;s a fascinating glimpse at what just might end up being generative AI&rsquo;s Achilles heel. If so, what does it all mean for regular people, the burgeoning AI industry, and the internet itself?</p><p><em>This interview has been edited for length and clarity.</em></p><p><em><strong>Futurism: So you coined a term for the phenomenon of AI self-consumption: MAD. Can you tell us what that acronym stands for, and what that phenomenon entails?</strong></em></p><p><strong>Richard G. Baraniuk:</strong> So, AI is a big field. Generative AI is one important part, and one that the public has become really aware of lately. Generative models generate or synthesize data. So in ChatGPT a person types in a prompt, then the GPT model synthesizes text to write a response. Or if you&rsquo;re using image generators like DALL-E or Stable Diffusion, you put in a text prompt, and the system generates a digital image.</p><p>So engineers develop these generative models. They&rsquo;re basically a computer program, and the program that needs to be trained. Right? And it needs to be trained with huge amounts of data from the internet. ChatGPT was trained on as much of the world wide web as OpenAI could find â€” basically everything. DALL-E was similarly trained on as many digital images out on the web that could be found.</p><p>And the crux is that increasingly, AI models are being trained on not just natural data, or real data sourced from the real world. Now, they&rsquo;re also being trained with data that&rsquo;s been synthesized by other generative models. So consequently, we&rsquo;ve entered an age where either wittingly or unwittingly, generative models are increasingly consuming the outputs from other generative models. Some companies are willingly training generative models on synthetic data, particularly when they&rsquo;re in an area where there just isn&rsquo;t enough real data. Some people are unwittingly training on generative models â€” for example, it turns out that a lot of the big training datasets that are used today for learning actually contain synthetic images generated by other generative models.</p><p>We summarize this in what we call an autophageous loop. That&rsquo;s a technical term that basically just means self-consuming. Maybe think of an animal, not just chasing his tail, but eating his tail. We also like the analogy to Mad Cow Disease â€” feeding cows to other young cows in an ever-repeating cycle that leads to brain-destroying pathogens. So basically, our work has been studying these self-consuming loops, and understanding when models go MAD, if you will. When bad things happen, and what to do if you don&rsquo;t want bad things to happen.</p><p><strong><em>Futurism: Is there a certain threshold where synthetic content starts to cause problems? As you&rsquo;ve said, synthetic content is already making its way into AI training sets. But how much synthetic content does it take for a model to go MAD and break down?</em></strong></p><p><em><strong>Josue Casco-Rodriguez:</strong></em> It definitely varies from model to model situation to situation.</p><p><em><strong>Sina Alemohammad:</strong></em> Let&rsquo;s look at this intuitively. Say you have 1 billion pieces of natural data, and you have one piece of synthetic data. MAD won&rsquo;t happen. But one year later, if you have 1 billion pieces of synthetic data, then definitely it&rsquo;ll go MAD in five iterations. We have found this ratio in Gaussian modeling.</p><p><em><strong>Baraniuk:</strong></em> Right. And just to be clear, there absolutely is a threshold for every model. But figuring out for DALL-E versus Midjourney, what the exact balance of real and synthetic data needs to be to keep everything safe and not going MAD, that&rsquo;s still a subject of research. But now the question for these big industrial models is, well, what is it?</p><p><em><strong>Futurism: So what are some of the implications for AI companies, then? We could say, ideally, that none of this ever gets into datasets. But that&rsquo;s obviously happened â€” after all, people are
<a href=https://futurism.com/the-byte/people-automating-responses-train-ai rel=noopener>already using AI to do Mechanical Turk work</a>.</strong></em></p><p><strong><em>Casco-Rodriguez:</em></strong> So when you&rsquo;re unwittingly using synthetic data â€” that also applies to practitioners, people who are generating images and putting them on the internet â€” you&rsquo;re probably not going to be conscious of the fact that what you produce is going to be in the future training of generative models. We see this with the dataset
<a href=https://laion.ai/blog/laion-5b/ rel=noopener>Laion-5B</a>, for example, which was used to train Stable Diffusion: generative images that people made in the past are being used to train new generative models. So if people are producing synthetic data, they need to be conscious of this fact. On the company side, your best shot is using something like watermarking to be able to detect synthetic data and maybe remove it.</p><p>As far as when you&rsquo;re knowingly using synthetic data, you need to be conscious that these generative models aren&rsquo;t perfect, and that they oftentimes do things like sacrifice synthetic diversity for synthetic quality. If that&rsquo;s happening in your model, and you&rsquo;re training on it, then you need to be cognizant that that&rsquo;s happening.</p><p><em><strong>Baraniuk:</strong></em> Say there are companies that, for whatever reason â€” maybe it&rsquo;s cheaper to use synthetic data, or they just don&rsquo;t have enough real data â€” and they just throw caution to the wind. They say, &ldquo;we&rsquo;re going to use synthetic data.&rdquo; What they don&rsquo;t realize is that if they do this generation after generation, one thing that&rsquo;s going to happen is the artifacts are going to be amplified. Your synthetic data is going to start to drift away from reality.</p><p>That&rsquo;s the thing that&rsquo;s really the most dangerous, and you might not even realize it&rsquo;s happening. And by drift away from reality, I mean you&rsquo;re generating images that are going to become increasingly, like, monotonous and dull. The same thing will happen for text as well if you do this â€” the diversity of the generated images is going to steadily go down. In one experiment that we ran, instead of artifacts getting amplified, the pictures all converge into basically the same person. It&rsquo;s totally freaky.</p><p><strong><em>Futurism: What are the implications for users of these systems?</em></strong></p><p><em><strong>Baraniuk:</strong></em> It&rsquo;s difficult for the users to protect themselves. If these models are being used in a loop like this, unfortunately, from a user perspective, the content they&rsquo;re generating is just going to become increasingly dull. And that&rsquo;s going to be disappointing, right? That&rsquo;s just fact.</p><p>So what can users really do to help the situation? One thing they can do is not turn off watermarking where it exists. There are some downsides to watermarking, but if someone&rsquo;s training a new model, they could seek synthetic images with watermarks and throw them out. That would really help with this threshold effect that we talked about. The second thing that users need to know is that their outputs, if they put them on the web, are going to invariably leak into training datasets for future systems. Some things are just inevitable.</p><p><em><strong>Futurism: What are the downsides of watermarking?</strong></em></p><p><em><strong>Baraniuk:</strong></em> It intentionally introduces an artifact. And compounded over generations, those can blow up like the AI-generated images in our paper.</p><p><em><strong>Alemohammad:</strong></em> Yeah, the problem is unknown. We don&rsquo;t know how the watermark can or will be amplified. But definitely, the benefits outweigh the downside. Right now, watermarking is the solution that we need to find synthetic data.</p><p><em><strong>Futurism:</strong> <strong>AI is already being integrated into services throughout the internet, most notably search engines. And search engines, to some capacity, are how we do almost everything online â€” they&rsquo;re central to the mediation and navigation of the web. Are you at all concerned about the future of the web&rsquo;s usability, if generated AI models are integrated into the web and into our daily lives, and then start to degrade because they keep swallowing synthetic material?</strong></em></p><p><em><strong>Baraniuk:</strong></em> This a really important long-term question. There&rsquo;s no question that MADness has the potential to significantly reduce the quality of the data on the internet. Just the quality of the data. And our work in this particular paper hasn&rsquo;t really dealt with the kind of AI systems used, let&rsquo;s say, in search engines. So it&rsquo;s a bit too early to tell. But there is some other work out there that actually shows that if you train a different, non-generative AI system â€” some other kind of AI system like the kind used in search engines â€” if you train that AI system using synthetic data in addition to real data, performance actually goes down.</p><p>So this supports the hypothesis that the more synthetic data that&rsquo;s out there, it could actually lower the performance of a whole host of tools, search engines included, that are trained on all of this data out there on the internet â€” some of which is real, but some of which is synthetic. Folks are starting to connect those dots.</p><p><strong><em>Casco-Rodriguez:</em></strong> One thought I&rsquo;ve had is that the ping-ponging back and forth between models can be really freaky. And since generative AI is already being used to do things like generate websites entirely, you could wind up having generative models leading you to results that are also synthetic, that have hyperlinks to other synthetic websites. There could be a whole synthetic ecosystem that you find through search engines, which is kind of crazy.</p><p><em><strong>Baraniuk:</strong></em> Yeah, you&rsquo;d get trapped in that world. It connects back to how people are using ChatGPT to do Mechanical Turk work. In order to do supervised learning â€” which is one of the big ways people learn from data to build these kinds of models â€” you need to label data. This kind of data annotation has been the gold standard, but now they&rsquo;re finding that when you put these labeling tasks out on a service like, say, Mechanical Turk, people aren&rsquo;t doing it anymore. They&rsquo;re just asking AI systems to do the labeling for them. It&rsquo;s more efficient, but it puts us exactly in another one of these loops we&rsquo;ve been talking about. Loops upon loops.</p><p><em><strong>Futurism:</strong></em> Snake eating its tail.</p><p><em><strong>Baraniuk:</strong></em> No question. Again, it&rsquo;s this idea of loops on top of loops, and that can make it extremely hard to ultimately track down the source of where any problems in AI models are coming from.</p><p>A quick story there: the whole jumping off point for our research happened when one of our group members was at a conference presenting a poster â€” not on this stuff, but on related work â€” and there was a researcher from industry who walked by who, just offhandedly, remarked that pretty soon, there&rsquo;s gonna be more synthetic images on the internet than real images. This was about a year and a half ago, and he said that there are going to be more synthetic websites than real websites. There&rsquo;s going to be more fake text than real text.</p><p><strong>More on AI:</strong>
<a href=https://futurism.com/the-byte/ai-synthetic-data rel=noopener><em>AI Developers Are Already Quietly Training AI Models Using AI-Generated Data</em></a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://williamechols.github.io/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by William Echols using <a href=/notes/Obsidian>Obsidian</a> and <a href=/notes/Hugo>Hugo</a></p><ul><li><a href=https://williamechols.github.io/>Home</a></li><li><a href=https://people.tamu.edu/~williamechols/#>Professional</a></li><li><a href=https://github.com/williamechols>GitHub</a></li><li><a href=https://linkedin.com/in/williamdechols>LinkedIn</a></li></ul></footer></div></div></body></html>